{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "Uncomment the following lines to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# Use the animation for pip install\n",
    "# loader = LoadingAnimation()\n",
    "# loader.start(\"Installing\")\n",
    "# %pip install -r requirements.txt -q\n",
    "# loader.stop(\"Installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 557
   },
   "outputs": [],
   "source": [
    "# Create reusable loading animation class\n",
    "import os, sys\n",
    "import time\n",
    "import threading\n",
    "\n",
    "class LoadingAnimation:\n",
    "    def __init__(self):\n",
    "        self.stop_event = threading.Event()\n",
    "        self.animation_thread = None\n",
    "\n",
    "    def _animate(self, message=\"Loading\"):\n",
    "        chars = \"/—\\\\|\"\n",
    "        while not self.stop_event.is_set():\n",
    "            for char in chars:\n",
    "                sys.stdout.write('\\r' + message + '... ' + char)\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(0.1)\n",
    "                if self.stop_event.is_set():\n",
    "                    sys.stdout.write(\"\\n\")\n",
    "                    break\n",
    "\n",
    "    def start(self, message=\"Loading\"):\n",
    "        self.stop_event.clear()\n",
    "        self.animation_thread = threading.Thread(target=self._animate, args=(message,))\n",
    "        self.animation_thread.daemon = True\n",
    "        self.animation_thread.start()\n",
    "\n",
    "    def stop(self, completion_message=\"Complete\"):\n",
    "        self.stop_event.set()\n",
    "        if self.animation_thread:\n",
    "            self.animation_thread.join()\n",
    "        print(f\"\\r{completion_message} ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Define a fake `load_dotenv` function\n",
    "def _load_dotenv(*args, **kwargs):\n",
    "    env_path = kwargs.get('dotenv_path', '.env')  # Default to '.env'\n",
    "    parsed_env = dotenv_values(env_path)\n",
    "\n",
    "    # Manually set valid key-value pairs\n",
    "    for key, value in parsed_env.items():\n",
    "        if key and value:  # Check for valid key-value pairs\n",
    "            os.environ[key] = value\n",
    "\n",
    "dotenv.load_dotenv = _load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and Setup\n",
    "Initial imports for the CrewAI Flow and Crew and setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from typing import Any, List\n",
    "import time\n",
    "\n",
    "# Importing Crew related components\n",
    "from crewai import LLM\n",
    "\n",
    "# Importing CrewAI Flow related components\n",
    "from crewai.flow import Flow, listen, start, persist, or_, router\n",
    "from crewai.flow.flow import FlowState\n",
    "\n",
    "# Apply a patch to allow nested asyncio loops in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing for Llama 3.3 Prompting Template\n",
    "\n",
    "When using different models the ability to go a lower level and change the prompting template can drastically improve the performance of the model, you want to make sure to watch for the model's training prompt patterns and adjust accordingly.\n",
    "\n",
    "For Meta's Llama you can find it [in here](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/#prompt-template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "# Agents Prompting Template for Llama 3.3\n",
    "system_template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{{ .System }}<|eot_id|>\"\"\"\n",
    "prompt_template=\"\"\"<|start_header_id|>user<|end_header_id|>{{ .Prompt }}<|eot_id|>\"\"\"\n",
    "response_template=\"\"\"<|start_header_id|>assistant<|end_header_id|>{{ .Response }}<|eot_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 1373
   },
   "outputs": [],
   "source": [
    "class ConversationalFlowState(FlowState):\n",
    "  \"\"\"\n",
    "  State for the conversational flow\n",
    "  \"\"\"\n",
    "  message: str = \"\"\n",
    "  query_result: List[Any] = []\n",
    "  conversation_history: List[Any] = []\n",
    "  step_timings: dict = {}\n",
    "  llm_call_time: float = 0\n",
    "  search_time: float = 0\n",
    "\n",
    "@persist()\n",
    "class ConversationalFlow(Flow[ConversationalFlowState]):\n",
    "  @start()\n",
    "  def start_conversation(self):\n",
    "    print(f\"# Starting conversation\\n\")\n",
    "    self.llm = LLM(model=\"groq/llama-3.3-70b-versatile\")\n",
    "    self.state.step_timings = {}\n",
    "    self.state.llm_call_time = 0\n",
    "    self.state.search_time = 0\n",
    "\n",
    "  @router(or_('start_conversation', 'answer_user_message'))\n",
    "  def listen_for_user_input(self):\n",
    "    start_time = time.time()\n",
    "    message = input(\"Enter your message: \")\n",
    "    if message.lower() == \"exit\":\n",
    "      pass\n",
    "    else:\n",
    "      self.state.message = message\n",
    "      self.state.conversation_history.append({\"role\": \"user\", \"content\": message})\n",
    "      self.state.step_timings['listen_for_user_input'] = time.time() - start_time\n",
    "      return 'message_received'\n",
    "\n",
    "  @router('message_received')\n",
    "  def process_user_input(self):\n",
    "    start_time = time.time()\n",
    "    messages = self.state.conversation_history.copy()\n",
    "    messages.append(\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"\"\"Check if you need more details about crewai enterprise features to answer.\n",
    "                    Only ask for more info if the question is not clearly about crewai.\n",
    "\n",
    "                    If you have enough info, just reply 'complete'.\n",
    "                    If you need more info, reply with one search sentence.\n",
    "\n",
    "                    Look at our chat history and my message.\n",
    "                    Decide if you can give a good answer with what you know.\"\"\"\n",
    "    })\n",
    "\n",
    "    llm_start = time.time()\n",
    "    response = self.llm.call(messages)\n",
    "    self.state.llm_call_time += time.time() - llm_start\n",
    "\n",
    "    if response == 'complete':\n",
    "      self.state.step_timings['process_user_input'] = time.time() - start_time\n",
    "      return 'answer'\n",
    "    else:\n",
    "      # something soon\n",
    "      return 'answer'\n",
    "\n",
    "\n",
    "  @listen('answer')\n",
    "  def answer_user_message(self):\n",
    "    start_time = time.time()\n",
    "    llm_start = time.time()\n",
    "    response = self.llm.call(self.state.conversation_history)\n",
    "    self.state.llm_call_time += time.time() - llm_start\n",
    "\n",
    "    self.state.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    print(f\"# Assistant response: {response}\\n\")\n",
    "    self.state.step_timings['answer_user_message'] = time.time() - start_time\n",
    "\n",
    "    print(f\"\\nTiming Summary:\")\n",
    "    print(f\"Total LLM call time: {self.state.llm_call_time:.2f}s\")\n",
    "    print(f\"Total Search time: {self.state.search_time:.2f}s\")\n",
    "    print(\"Step timings:\")\n",
    "    for step, timing in self.state.step_timings.items():\n",
    "        print(f\"  {step}: {timing:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "flow = ConversationalFlow()\n",
    "flow.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "flow_new = ConversationalFlow()\n",
    "flow_new.kickoff(inputs={\"id\": flow.flow_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
