{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Code Documentation Agents with CrewAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to create a crew of multiple Agents for writing documentation using CrewAI Flows.\n",
    "\n",
    "The crew will analyze code from any public GitHub repository and generate comprehensive documentation\n",
    "by working collaboratively using specialized agents with different roles and responsibilities.\n",
    "\n",
    "CrewAI Flows enable coordinated execution and communication between agents to produce high-quality\n",
    "documentation for any codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "height": 574
   },
   "outputs": [],
   "source": [
    "# Create reusable loading animation class\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import threading\n",
    "\n",
    "class LoadingAnimation:\n",
    "    def __init__(self):\n",
    "        self.stop_event = threading.Event()\n",
    "        self.animation_thread = None\n",
    "\n",
    "    def _animate(self, message=\"Loading\"):\n",
    "        chars = \"/—\\\\|\"\n",
    "        while not self.stop_event.is_set():\n",
    "            for char in chars:\n",
    "                sys.stdout.write('\\r' + message + '... ' + char)\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(0.1)\n",
    "                if self.stop_event.is_set():\n",
    "                    sys.stdout.write(\"\\n\")\n",
    "                    break\n",
    "\n",
    "    def start(self, message=\"Loading\"):\n",
    "        self.stop_event.clear()\n",
    "        self.animation_thread = threading.Thread(target=self._animate, args=(message,))\n",
    "        self.animation_thread.daemon = True\n",
    "        self.animation_thread.start()\n",
    "\n",
    "    def stop(self, completion_message=\"Complete\"):\n",
    "        self.stop_event.set()\n",
    "        if self.animation_thread:\n",
    "            self.animation_thread.join()\n",
    "        print(f\"\\r{completion_message} ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "Uncomment the following lines to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Use the animation for pip install\n",
    "# loader = LoadingAnimation()\n",
    "# loader.start(\"Installing\")\n",
    "%pip install -r requirements.txt -q\n",
    "# loader.stop(\"Installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import threading\n",
    "\n",
    "class LoadingAnimation:\n",
    "    def __init__(self):\n",
    "        self.stop_event = threading.Event()\n",
    "        self.animation_thread = None\n",
    "\n",
    "    def _animate(self, message=\"Loading\"):\n",
    "        chars = \"/—\\\\|\"\n",
    "        while not self.stop_event.is_set():\n",
    "            for char in chars:\n",
    "                sys.stdout.write('\\r' + message + '... ' + char)\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(0.1)\n",
    "                if self.stop_event.is_set():\n",
    "                    sys.stdout.write(\"\\n\")\n",
    "                    break\n",
    "\n",
    "    def start(self, message=\"Loading\"):\n",
    "        self.stop_event.clear()\n",
    "        self.animation_thread = threading.Thread(target=self._animate, args=(message,))\n",
    "        self.animation_thread.daemon = True\n",
    "        self.animation_thread.start()\n",
    "\n",
    "    def stop(self, completion_message=\"Complete\"):\n",
    "        self.stop_event.set()\n",
    "        if self.animation_thread:\n",
    "            self.animation_thread.join()\n",
    "        print(f\"\\r{completion_message} ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Define a fake `load_dotenv` function\n",
    "def _load_dotenv(*args, **kwargs):\n",
    "    env_path = kwargs.get('dotenv_path', '.env')  # Default to '.env'\n",
    "    parsed_env = dotenv_values(env_path)\n",
    "\n",
    "    # Manually set valid key-value pairs\n",
    "    for key, value in parsed_env.items():\n",
    "        if key and value:  # Check for valid key-value pairs\n",
    "            os.environ[key] = value\n",
    "\n",
    "dotenv.load_dotenv = _load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and Setup\n",
    "Initial imports for the CrewAI Flow and Crew and setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import yaml\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Importing Crew related components\n",
    "from crewai import Agent, Task, Crew\n",
    "\n",
    "# Importing CrewAI Flow related components\n",
    "from crewai.flow.flow import Flow, listen, start\n",
    "\n",
    "# Apply a patch to allow nested asyncio loops in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the project URL\n",
    "\n",
    "In this demo, a sample repository is provided for you. However, feel fry to test this on other public repositories! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "project_url = \"https://github.com/crewAIInc/crewAI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Application Mapper Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 625
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "class Component(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    file_path: Optional[str] = None\n",
    "\n",
    "class RelevantFile(BaseModel):\n",
    "    path: str\n",
    "    description: str\n",
    "\n",
    "class ProjectReport(BaseModel):\n",
    "    project_overview: str\n",
    "    project_organization: str\n",
    "    main_components: List[Component]\n",
    "    relevant_files: List[RelevantFile]\n",
    "    additional_information: Optional[str] = None\n",
    "\n",
    "\n",
    "class ComponentFile(BaseModel):\n",
    "    path: str\n",
    "    description: str\n",
    "\n",
    "class ComponentDetails(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    files_and_directories: List[ComponentFile]\n",
    "    main_functionality: str\n",
    "    integration: str\n",
    "    additional_info: Optional[str] = None\n",
    "\n",
    "class ComponentsReport(BaseModel):\n",
    "    components: List[ComponentDetails]\n",
    "    other_relevant_info: Optional[str] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 642
   },
   "outputs": [],
   "source": [
    "from crewai_tools import (\n",
    "    DirectoryReadTool,\n",
    "    FileReadTool,\n",
    ")\n",
    "\n",
    "# Load agent and task configurations from YAML files\n",
    "with open('config/application_mapper_agents.yaml', 'r') as f:\n",
    "    agents_config = yaml.safe_load(f)\n",
    "\n",
    "with open('config/application_mapper_tasks.yaml', 'r') as f:\n",
    "    tasks_config = yaml.safe_load(f)\n",
    "\n",
    "principal_engineer = Agent(\n",
    "  config=agents_config['principal_engineer'],\n",
    "  tools=[\n",
    "    DirectoryReadTool(),\n",
    "    FileReadTool()\n",
    "  ]\n",
    ")\n",
    "\n",
    "understand_application = Task(\n",
    "  config=tasks_config['understand_application'],\n",
    "  agent=principal_engineer,\n",
    "  output_pydantic=ProjectReport\n",
    ")\n",
    "\n",
    "map_components = Task(\n",
    "  config=tasks_config['map_components'],\n",
    "  agent=principal_engineer,\n",
    "  output_pydantic=ComponentsReport\n",
    ")\n",
    "\n",
    "application_mapper_crew = Crew(\n",
    "    agents=[principal_engineer],\n",
    "    tasks=[understand_application, map_components],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 693
   },
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "from crewai.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ComponentInfoInput(BaseModel):\n",
    "    \"\"\"Input schema for ComponentInfoTool.\"\"\"\n",
    "    component_name: str = Field(..., description=\"Name of the component to get information about\")\n",
    "\n",
    "class ComponentInfoTool(BaseTool):\n",
    "    name: str = \"Component Information Tool\"\n",
    "    description: str = \"Gets detailed information about a specific component from the components report\"\n",
    "    args_schema: Type[BaseModel] = ComponentInfoInput\n",
    "    components_report: BaseModel = Field(None, description=\"Components report containing component information\")\n",
    "\n",
    "    def __init__(self, components_report):\n",
    "        super().__init__()\n",
    "        self.components_report = components_report\n",
    "\n",
    "    def _run(self, component_name: str) -> str:\n",
    "        # Find the component in the components array\n",
    "        component = next((c for c in self.components_report.components if c.name.lower() == component_name.lower()), None)\n",
    "\n",
    "        if not component:\n",
    "            return f\"Component '{component_name}' not found\"\n",
    "\n",
    "        # Format component information as string\n",
    "        info = f\"\"\"\n",
    "Component: {component.name}\n",
    "Description: {component.description}\n",
    "Main Functionality: {component.main_functionality}\n",
    "Integration: {component.integration}\n",
    "Additional Info: {component.additional_info}\n",
    "\n",
    "Files and Directories:\n",
    "\"\"\"\n",
    "        for file in component.files_and_directories:\n",
    "            info += f\"- {file.path}: {file.description}\\n\"\n",
    "\n",
    "        return info.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Planning Crew\n",
    "\n",
    "Initial strucutre data we will use to capture the output of the planning crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define data structures to capture documentation planning output\n",
    "class DocSection(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "    content: str\n",
    "\n",
    "class DocItem(BaseModel):\n",
    "    \"\"\"Represents a documentation item\"\"\"\n",
    "    title: str\n",
    "    sections: list[DocSection]\n",
    "    prerequisites: str\n",
    "    associated_entities: list[str]\n",
    "    examples: list[str]\n",
    "    goal: str\n",
    "\n",
    "class DocPlan(BaseModel):\n",
    "    \"\"\"Documentation plan\"\"\"\n",
    "    docs: list[DocItem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 744
   },
   "outputs": [],
   "source": [
    "from crewai_tools import (\n",
    "    DirectoryReadTool,\n",
    "    FileReadTool,\n",
    ")\n",
    "\n",
    "# Load agent and task configurations from YAML files\n",
    "with open('config/planner_agents.yaml', 'r') as f:\n",
    "    agents_config = yaml.safe_load(f)\n",
    "\n",
    "with open('config/planner_tasks.yaml', 'r') as f:\n",
    "    tasks_config = yaml.safe_load(f)\n",
    "\n",
    "code_explorer = Agent(\n",
    "  config=agents_config['code_explorer'],\n",
    "  tools=[\n",
    "    DirectoryReadTool(),\n",
    "    FileReadTool()\n",
    "  ]\n",
    ")\n",
    "\n",
    "documentation_planner = Agent(\n",
    "  config=agents_config['documentation_planner'],\n",
    "  tools=[\n",
    "    DirectoryReadTool(),\n",
    "    FileReadTool()\n",
    "  ]\n",
    ")\n",
    "\n",
    "analyze_codebase = Task(\n",
    "  config=tasks_config['analyze_codebase'],\n",
    "  agent=code_explorer\n",
    ")\n",
    "create_documentation_plan = Task(\n",
    "  config=tasks_config['create_documentation_plan'],\n",
    "  agent=documentation_planner,\n",
    "  output_pydantic=DocPlan\n",
    ")\n",
    "\n",
    "planning_crew = Crew(\n",
    "    agents=[code_explorer, documentation_planner],\n",
    "    tasks=[analyze_codebase, create_documentation_plan],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Documentation Crew\n",
    "\n",
    "Crew of AI Agents to execute the documentation plan and create the documentation.\n",
    "Creating a guardrail to check the mermaid syntax in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 387
   },
   "outputs": [],
   "source": [
    "from crewai.tasks import TaskOutput\n",
    "import re\n",
    "\n",
    "def check_mermaid_syntax(task_output: TaskOutput):\n",
    "    text = task_output.raw\n",
    "\n",
    "    # Find all mermaid code blocks in the text\n",
    "    mermaid_blocks = re.findall(r'```mermaid\\n(.*?)\\n```', text, re.DOTALL)\n",
    "\n",
    "    for block in mermaid_blocks:\n",
    "        diagram_text = block.strip()\n",
    "        lines = diagram_text.split('\\n')\n",
    "        corrected_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "            corrected_line = re.sub(r'\\|.*?\\|>', lambda match: match.group(0).replace('|>', '|'), line)\n",
    "            corrected_lines.append(corrected_line)\n",
    "\n",
    "        text = text.replace(block, \"\\n\".join(corrected_lines))\n",
    "\n",
    "    task_output.raw = text\n",
    "    return (True, task_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 710
   },
   "outputs": [],
   "source": [
    "from crewai_tools import (\n",
    "    DirectoryReadTool,\n",
    "    FileReadTool,\n",
    "    WebsiteSearchTool\n",
    ")\n",
    "\n",
    "# Load agent and task configurations from YAML files\n",
    "with open('config/documentation_agents.yaml', 'r') as f:\n",
    "    agents_config = yaml.safe_load(f)\n",
    "\n",
    "with open('config/documentation_tasks.yaml', 'r') as f:\n",
    "    tasks_config = yaml.safe_load(f)\n",
    "\n",
    "overview_writer = Agent(config=agents_config['overview_writer'], tools=[\n",
    "    DirectoryReadTool(),\n",
    "    FileReadTool(),\n",
    "    WebsiteSearchTool(website=\"https://mermaid.js.org/intro/\")\n",
    "])\n",
    "\n",
    "documentation_reviewer = Agent(config=agents_config['documentation_reviewer'], tools=[\n",
    "    DirectoryReadTool(directory=\"docs/\", name=\"Check existing documentation folder\"),\n",
    "    FileReadTool(),\n",
    "])\n",
    "\n",
    "draft_documentation = Task(\n",
    "  config=tasks_config['draft_documentation'],\n",
    "  agent=overview_writer\n",
    ")\n",
    "\n",
    "qa_review_documentation = Task(\n",
    "  config=tasks_config['qa_review_documentation'],\n",
    "  agent=documentation_reviewer,\n",
    "  guardrail=check_mermaid_syntax,\n",
    "  max_retries=5\n",
    ")\n",
    "\n",
    "documentation_crew = Crew(\n",
    "    agents=[overview_writer, documentation_reviewer],\n",
    "    tasks=[draft_documentation, qa_review_documentation],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Documentation Flow\n",
    "\n",
    "A Flow to create the documentation for the project where we will use the planning crew to plan the documentation and the documentation crew to create the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 1900
   },
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class DocumentationState(BaseModel):\n",
    "  \"\"\"\n",
    "  State for the documentation flow\n",
    "  \"\"\"\n",
    "  project_url: str = project_url\n",
    "  repo_path: Path = \"workdir/\"\n",
    "  docs: List[str] = []\n",
    "  project_report: ProjectReport | None = None\n",
    "  components_report: ComponentsReport | None = None\n",
    "\n",
    "class CreateDocumentationFlow(Flow[DocumentationState]):\n",
    "  # Clone the repository, initial step\n",
    "  # No need for AI Agents on this step, so we just use regular Python code\n",
    "  @start()\n",
    "  def clone_repo(self):\n",
    "    print(f\"# Cloning repository: {self.state.project_url}\\n\")\n",
    "    # Extract repo name from URL\n",
    "    repo_name = self.state.project_url.split(\"/\")[-1]\n",
    "    self.state.repo_path = f\"{self.state.repo_path}{repo_name}\"\n",
    "\n",
    "  # Check if directory exists\n",
    "    if Path(self.state.repo_path).exists():\n",
    "      print(f\"# Repository directory already exists at {self.state.repo_path}\\n\")\n",
    "      subprocess.run([\"rm\", \"-rf\", self.state.repo_path])\n",
    "      print(\"# Removed existing directory\\n\")\n",
    "\n",
    "    # Clone the repository\n",
    "    subprocess.run([\"git\", \"clone\", self.state.project_url, self.state.repo_path])\n",
    "    return self.state\n",
    "\n",
    "  @listen(clone_repo)\n",
    "  def map_project_structure(self):\n",
    "    print(f\"# Mapping project structure for: {self.state.repo_path}\\n\")\n",
    "    result = application_mapper_crew.kickoff(inputs={'repo_path': self.state.repo_path})\n",
    "    print(f\"# Project structure for {self.state.repo_path}:\")\n",
    "    for component in result.tasks_output[1].pydantic.components:\n",
    "        print(f\"    - {component.name}\")\n",
    "    self.state.project_report = result.tasks_output[0].pydantic\n",
    "    self.state.components_report = result.tasks_output[1].pydantic\n",
    "\n",
    "  @listen(map_project_structure)\n",
    "  def plan_docs(self):\n",
    "    print(f\"# Planning documentation for: {self.state.repo_path}\\n\")\n",
    "\n",
    "    tool = ComponentInfoTool(components_report=self.state.components_report)\n",
    "    planning_crew.agents[0].tools.append(tool)\n",
    "    planning_crew.agents[1].tools.append(tool)\n",
    "\n",
    "    result = planning_crew.kickoff(inputs={\n",
    "      'repo_path': self.state.repo_path,\n",
    "      'project_overview': self.state.project_report.project_overview,\n",
    "      'project_organization': self.state.project_report.project_organization,\n",
    "      'additional_information': self.state.project_report.additional_information,\n",
    "      'components_names': [c.name for c in self.state.components_report.components]\n",
    "    })\n",
    "    print(f\"# Planned docs for {self.state.repo_path}:\")\n",
    "    for doc in result.pydantic.docs:\n",
    "        print(f\"    - {doc.title}\")\n",
    "    return result\n",
    "\n",
    "  @listen(plan_docs)\n",
    "  def save_plan(self, plan):\n",
    "    with open(\"docs/plan.json\", \"w\") as f:\n",
    "      f.write(plan.raw)\n",
    "\n",
    "  @listen(plan_docs)\n",
    "  def create_docs(self, plan):\n",
    "    for doc in plan.pydantic.docs:\n",
    "      print(f\"\\n# Creating documentation for: {doc.title}\")\n",
    "\n",
    "      # Save documentation to file in docs folder\n",
    "      docs_dir = Path(\"docs\")\n",
    "      docs_dir.mkdir(exist_ok=True)\n",
    "      title = doc.title.lower().replace(\" \", \"_\") + \".mdx\"\n",
    "      documentation = []\n",
    "\n",
    "      with open(docs_dir / title, \"w\") as f:\n",
    "        for section in doc.sections:\n",
    "          print(f\"  ## Writing section: {section.title}\")\n",
    "          retries = 3\n",
    "          for attempt in range(retries):\n",
    "              try:\n",
    "                  result = documentation_crew.kickoff(inputs={\n",
    "                      'title': doc.title,\n",
    "                      'repo_path': self.state.repo_path,\n",
    "                      'project_overview': self.state.project_report.project_overview,\n",
    "                      'project_organization': self.state.project_report.project_organization,\n",
    "                      'additional_information': self.state.project_report.additional_information,\n",
    "                      'section': section.title,\n",
    "                      'description': section.description,\n",
    "                      'prerequisites': doc.prerequisites,\n",
    "                      'associated_entities': doc.associated_entities,\n",
    "                      'examples': '\\n'.join(doc.examples),\n",
    "                      'documentation': '\\n'.join(documentation) if documentation else 'this is the first section',\n",
    "                      'goal': doc.goal\n",
    "                  })\n",
    "                  documentation.append(result.raw)\n",
    "                  break\n",
    "              except Exception as e:\n",
    "                  if attempt == retries - 1:  # Last attempt\n",
    "                      print(f\"Error creating documentation for {section.title} after {retries} attempts: {e}\")\n",
    "                  else:\n",
    "                      print(f\"Attempt {attempt + 1} failed for {section.title}: {e}. Retrying...\")\n",
    "\n",
    "        self.state.docs.append(str(docs_dir / title))\n",
    "        f.write(\"\\n\".join(documentation))\n",
    "    print(f\"\\n# Documentation created for: {self.state.repo_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing helper methods to plot and execute the flow in a Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "# Plot the flow\n",
    "flow = CreateDocumentationFlow()\n",
    "flow.plot()\n",
    "\n",
    "# Display the flow visualization using IFrame\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Display the flow visualization\n",
    "IFrame(src='./crewai_flow.html', width='100%', height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Documentation Flow\n",
    "\n",
    "After running this cell, check the `docs` directory for the generated documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "flow = CreateDocumentationFlow()\n",
    "flow.kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot One of the Documents\n",
    "\n",
    "Let's visualize one of the generated documentation files to verify the output. This will help us ensure the documentation was created successfully and formatted correctly.\n",
    "\n",
    "The generated documentation files can be found in the `docs` directory in the root of the project. Each documentation file is saved with a `.mdx` extension and follows the naming convention of lowercase words separated by underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "# List all files in docs folder and display the first doc using IPython.display\n",
    "from IPython.display import Markdown\n",
    "import pathlib\n",
    "\n",
    "docs_dir = pathlib.Path(\"docs\")\n",
    "print(\"Documentation files generated:\")\n",
    "for doc_file in docs_dir.glob(\"*.mdx\"):\n",
    "    print(f\"- docs/{doc_file.name}\")\n",
    "\n",
    "print(\"\\nDisplaying contents of first doc:\\n\")\n",
    "first_doc = pathlib.Path(flow.state.docs[0]).read_text()\n",
    "display(Markdown(first_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
